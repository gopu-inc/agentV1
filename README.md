---
language:
- fr
- en
license: mit
tags:
- text-generation
- conversational
- artificial-intelligence
- gopuAI
- agentV1
pipeline_tag: text-generation
widget:
- text: Bonjour, qui es-tu ?
  example_title: Pr√©sentation
- text: Explique-moi l'IA g√©n√©rative
  example_title: Explication IA
- text: Comment programmer en Python ?
  example_title: Aide programmation
datasets:
- unknown
metrics:
- accuracy
model-index:
- name: agentV1
  results:
  - task:
      name: Text Generation
      type: text-generation
    dataset:
      name: Custom Training Data
      type: unknown
    metrics:
    - name: Accuracy
      type: accuracy
      value: 0
base_model:
- microsoft/Phi-3-mini-4k-instruct
new_version: Gopu-poss/gopu-agent-2k-fdf
library_name: transformers
---
# ü§ñ agentV1 - Intelligence Artificielle Avanc√©e

**agentV1** est un mod√®le d'intelligence artificielle de pointe d√©velopp√© par **Mauricio Mangituka** pour **gopuAI**. Bas√© sur Microsoft Phi-3-mini-4k-instruct, ce mod√®le combine performance optimale et efficacit√© m√©moire.

![Version](https://img.shields.io/badge/version-1.0.0-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Hugging Face](https://img.shields.io/badge/Hugging%20Face-gopu--poss%2Fagent-yellow)
![GitHub](https://img.shields.io/badge/GitHub-gopu--inc%2FagentV1-black)

## üöÄ Caract√©ristiques

- **üß† Mod√®le de base**: Microsoft Phi-3-mini-4k-instruct
- **üíæ Taille compacte**: ~2-3 Go seulement
- **‚ö° Performances**: Excellentes capacit√©s de raisonnement
- **üåç Multilingue**: Support du fran√ßais et de l'anglais
- **üîß Optimis√©**: Quantification et optimisation m√©moire

## üìã Table des Mati√®res

- [Installation](#installation)
- [Utilisation Rapide](#utilisation-rapide)
- [API Compl√®te](#api-compl√®te)
- [Exemples](#exemples)
- [Architecture](#architecture)
- [D√©ploiement](#d√©ploiement)
- [Contribuer](#contribuer)
- [License](#license)
- [Contact](#contact)

## üõ† Installation

### Pr√©requis
- Python 3.8+
- PyTorch 2.0+
- Transformers 4.25+

### Installation des d√©pendances

```bash
pip install transformers torch accelerate
```

Installation directe

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("gopu-poss/agent")
model = AutoModelForCausalLM.from_pretrained(
    "gopu-poss/agent",
    torch_dtype=torch.float16,
    device_map="auto"
)
```

üöÄ Utilisation Rapide

Code minimal

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Chargement du mod√®le
tokenizer = AutoTokenizer.from_pretrained("gopu-poss/agent")
model = AutoModelForCausalLM.from_pretrained(
    "gopu-poss/agent",
    torch_dtype=torch.float16,
    device_map="auto"
)

# G√©n√©ration de texte
prompt = "Explique-moi comment fonctionne l'IA g√©n√©rative"
inputs = tokenizer(prompt, return_tensors="pt")

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.7,
        do_sample=True
    )

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

üîå API Compl√®te

Classe AgentV1

```python
class AgentV1:
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained("gopu-poss/agent")
        self.model = AutoModelForCausalLM.from_pretrained(
            "gopu-poss/agent",
            torch_dtype=torch.float16,
            device_map="auto"
        )
    
    def ask(self, question, max_tokens=200, temperature=0.7):
        """Pose une question √† l'agent"""
        inputs = self.tokenizer(question, return_tensors="pt")
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    def batch_ask(self, questions, max_tokens=200):
        """Pose plusieurs questions en lot"""
        responses = []
        for question in questions:
            responses.append(self.ask(question, max_tokens))
        return responses
```

üìö Exemples

Conversation basique

```python
agent = AgentV1()

# Question simple
response = agent.ask("Bonjour, qui es-tu ?")
print(response)
```

G√©n√©ration cr√©ative

```python
story = agent.ask(
    "√âcris une courte histoire sur un robot qui apprend l'√©motion",
    max_tokens=300,
    temperature=0.8
)
```

Assistance technique

```python
code_help = agent.ask(
    "Explique-moi comment trier une liste en Python",
    max_tokens=150
)
```

Analyse de texte

```python
analysis = agent.ask(
    "R√©sume les avantages de l'IA g√©n√©rative en 3 points",
    max_tokens=100
)
```

üèó Architecture

Mod√®le de Base

¬∑ Architecture: Transformer-based
¬∑ Param√®tres: 3.8 milliards
¬∑ Context Window: 4K tokens
¬∑ Pr√©-entra√Ænement: Texte multilingue

Optimisations

¬∑ Quantification: FP16 pour performance m√©moire
¬∑ Device Mapping: Chargement automatique GPU/CPU
¬∑ Gestion m√©moire: Optimis√©e pour usage efficace

üåê D√©ploiement

Sur GPU local

```python
model = AutoModelForCausalLM.from_pretrained(
    "gopu-poss/agent",
    torch_dtype=torch.float16,
    device_map="cuda:0"
)
```

Sur CPU

```python
model = AutoModelForCausalLM.from_pretrained(
    "gopu-poss/agent",
    torch_dtype=torch.float32,
    device_map="cpu"
)
```

Avec Docker

```dockerfile
FROM python:3.9-slim
RUN pip install transformers torch accelerate
COPY . /app
WORKDIR /app
CMD ["python", "app.py"]
```

üìä Performances

M√©triques

¬∑ Vitesse d'inf√©rence: ~50-100 tokens/seconde sur GPU
¬∑ Utilisation m√©moire: ~3-4 Go en FP16
¬∑ Latence: < 2 secondes pour 200 tokens

Cas d'Usage Recommand√©s

¬∑ ‚úÖ Assistance conversationnelle
¬∑ ‚úÖ G√©n√©ration de contenu
¬∑ ‚úÖ R√©ponse √† questions
¬∑ ‚úÖ Analyse de texte
¬∑ ‚úÖ Aide √† la programmation

ü§ù Contribuer

Nous accueillons les contributions ! Voici comment participer :

1. Fork le projet
2. Clone votre fork
3. Cr√©ez une branche (git checkout -b feature/AmazingFeature)
4. Commit vos changements (git commit -m 'Add AmazingFeature')
5. Push (git push origin feature/AmazingFeature)
6. Ouvrez une Pull Request

Standards de Code

¬∑ Utilisez Black pour le formatage
¬∑ √âcrivez des docstrings compl√®tes
¬∑ Ajoutez des tests pour les nouvelles fonctionnalit√©s

üìù License

Ce projet est sous licence MIT. Voir le fichier LICENSE pour plus de d√©tails.

üë®‚Äçüíª Cr√©ateur

Mauricio Mangituka

¬∑ GitHub: @gopu-inc
¬∑ Hugging Face: gopu-poss
¬∑ Email: mauricio@example.com

üè¢ Soci√©t√©

gopuAI - Innovation en Intelligence Artificielle
D√©veloppement de solutions IA accessibles et performantes

üîó Liens Importants

¬∑ ü§ó Hugging Face: gopu-poss/agent
¬∑ üêô GitHub: gopu-inc/agentV1
¬∑ üìö Documentation: Lien vers documentation
¬∑ üêõ Issues: GitHub Issues

üìû Support

¬∑ Questions techniques: Ouvrez une issue sur GitHub
¬∑ Collaborations: Contactez-nous par email
¬∑ Suggestions: Nous appr√©cions vos retours !

---

<div align="center">‚≠ê N'oubliez pas de donner une √©toile au projet si vous l'aimez !

D√©velopp√© avec ‚ù§Ô∏è par Mauricio Mangituka pour gopuAI

</div>
```Fichier additionnel : requirements.txt

```txt
torch>=2.0.0
transformers>=4.25.0
accelerate>=0.20.0
numpy>=1.21.0
safetensors>=0.3.0
```

Fichier additionnel : setup.py

```python
from setuptools import setup, find_packages

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

setup(
    name="agentv1",
    version="1.0.0",
    author="Mauricio Mangituka",
    author_email="mauricio@example.com",
    description="AgentV1 - Mod√®le IA avanc√© par gopuAI",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/gopu-inc/agentV1",
    packages=find_packages(),
    classifiers=[
        "Development Status :: 4 - Beta",
        "Intended Audience :: Developers",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
    ],
    python_requires=">=3.8",
    install_requires=[
        "torch>=2.0.0",
        "transformers>=4.25.0",
        "accelerate>=0.20.0",
    ],
)
```