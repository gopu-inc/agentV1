{"config":{"lang":["fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"agentV1 \u2014 Documentation Officielle","text":"<p>Bienvenue dans la documentation officielle du mod\u00e8le agentV1, d\u00e9velopp\u00e9 par Mauricio Mangituka pour gopuAI.</p> <p>agentV1 est un mod\u00e8le l\u00e9ger, rapide et optimis\u00e9, reposant sur Microsoft Phi-3-mini-4k-instruct. Il offre des capacit\u00e9s avanc\u00e9es en g\u00e9n\u00e9ration de texte, conversation, analyse et assistance technique.</p>"},{"location":"#fonctionnalites-principales","title":"\ud83d\ude80 Fonctionnalit\u00e9s principales","text":"<ul> <li>Mod\u00e8le l\u00e9ger (~2-3 Go)</li> <li>Multilingue (FR/EN)</li> <li>Raisonnement am\u00e9lior\u00e9</li> <li>Optimisation m\u00e9moire (FP16)</li> <li>Utilisation facile via Transformers</li> <li>API simple d\u2019utilisation</li> <li>D\u00e9ploiement rapide (Docker, Python, CI/CD)</li> </ul>"},{"location":"#contenu-de-la-documentation","title":"\ud83d\udcd8 Contenu de la documentation","text":"<ul> <li>Installation</li> <li>Guide d\u2019utilisation</li> <li>API compl\u00e8te</li> <li>Exemples pratiques</li> <li>Architecture du mod\u00e8le</li> <li>D\u00e9ploiement (CPU, GPU, Docker)</li> <li>FAQ</li> <li>Ressources techniques</li> </ul> <p>D\u00e9velopp\u00e9 avec \u2764\ufe0f par la communaut\u00e9 gopuAI.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequis","title":"Pr\u00e9requis","text":"<ul> <li>Python 3.8+</li> <li>PyTorch 2.0+</li> <li>Transformers 4.25+</li> <li>accelerate (optionnel)</li> </ul>"},{"location":"installation/#installation-des-dependances","title":"Installation des d\u00e9pendances","text":"<pre><code>pip install torch transformers accelerate\n</code></pre>"},{"location":"installation/#telecharger-agentv1","title":"T\u00e9l\u00e9charger agentV1","text":"<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"gopu-poss/agentV1\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"gopu-poss/agentV1\",\n    device_map=\"auto\",\n    torch_dtype=\"auto\"\n)\nV\u00e9rifier l\u2019installation\nprint(model.config)\n</code></pre>"},{"location":"installation/#verification","title":"verification*","text":"<p>```python print(model.config)</p>"},{"location":"usage/","title":"\ud83d\udcc4 3. <code>docs/usage.md</code>","text":""},{"location":"usage/#utilisation-rapide","title":"Utilisation Rapide","text":""},{"location":"usage/#generation-simple","title":"G\u00e9n\u00e9ration simple","text":"<pre><code>prompt = \"Explique-moi l'IA g\u00e9n\u00e9rative\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=200,\n    temperature=0.7\n)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nMode conversation\nhistory = []\nquestion = \"Bonjour, qui es-tu ?\"\n\nresponse = agent.ask(question)\nParam\u00e8tres importants\nParam\u00e8tre   Description\nmax_new_tokens  Nombre de tokens g\u00e9n\u00e9r\u00e9s\ntemperature Cr\u00e9ativit\u00e9\ntop_p   Filtrage nucleus\ndo_sample   G\u00e9n\u00e9ration al\u00e9atoire\nConseils d\u2019optimisation\nUtiliser torch.float16 sur GPU\nLimiter max_new_tokens sur CPU\nPr\u00e9f\u00e9rer do_sample=True pour la cr\u00e9ativit\u00e9\n</code></pre>"},{"location":"usage/#mode-conversation","title":"Mode conversation","text":"<pre><code>history = []\nquestion = \"Bonjour, qui es-tu ?\"\n\nresponse = agent.ask(question)\nParam\u00e8tres importants\nParam\u00e8tre   Description\nmax_new_tokens  Nombre de tokens g\u00e9n\u00e9r\u00e9s\ntemperature Cr\u00e9ativit\u00e9\ntop_p   Filtrage nucleus\ndo_sample   G\u00e9n\u00e9ration al\u00e9atoire\nConseils d\u2019optimisation\nUtiliser torch.float16 sur GPU\nLimiter max_new_tokens sur CPU\nPr\u00e9f\u00e9rer do_sample=True pour la cr\u00e9ativit\u00e9\n</code></pre>"}]}