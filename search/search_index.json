{"config":{"lang":["fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"agentV1 \u2014 Documentation Officielle","text":"<p>Bienvenue dans la documentation officielle du mod\u00e8le agentV1, d\u00e9velopp\u00e9 par Mauricio Mangituka pour gopuAI.</p> <p>agentV1 est un mod\u00e8le l\u00e9ger, rapide et optimis\u00e9, reposant sur Microsoft Phi-3-mini-4k-instruct. Il offre des capacit\u00e9s avanc\u00e9es en g\u00e9n\u00e9ration de texte, conversation, analyse et assistance technique.</p>"},{"location":"#fonctionnalites-principales","title":"\ud83d\ude80 Fonctionnalit\u00e9s principales","text":"<ul> <li>Mod\u00e8le l\u00e9ger (~2-3 Go)</li> <li>Multilingue (FR/EN)</li> <li>Raisonnement am\u00e9lior\u00e9</li> <li>Optimisation m\u00e9moire (FP16)</li> <li>Utilisation facile via Transformers</li> <li>API simple d\u2019utilisation</li> <li>D\u00e9ploiement rapide (Docker, Python, CI/CD)</li> </ul>"},{"location":"#contenu-de-la-documentation","title":"\ud83d\udcd8 Contenu de la documentation","text":"<ul> <li>Installation</li> <li>Guide d\u2019utilisation</li> <li>API compl\u00e8te</li> <li>Exemples pratiques</li> <li>Architecture du mod\u00e8le</li> <li>D\u00e9ploiement (CPU, GPU, Docker)</li> <li>FAQ</li> <li>Ressources techniques</li> </ul> <p>D\u00e9velopp\u00e9 avec \u2764\ufe0f par la communaut\u00e9 gopuAI.</p>"},{"location":"api/","title":"<code>docs/api.md</code>","text":""},{"location":"api/#api-de-agentv1","title":"API de agentV1","text":""},{"location":"api/#classe-principale-agentv1","title":"Classe principale : AgentV1","text":""},{"location":"api/#initialisation","title":"Initialisation","text":"<p>```python</p> <p>agent = AgentV1() M\u00e9thode : ask() Pose une question \u00e0 l\u2019agent. agent.ask(     question=\"Explique-moi Python.\",     max_tokens=200,     temperature=0.7 ) M\u00e9thode : batch_ask() agent.batch_ask([     \"Bonjour\",     \"Explique-moi les listes en Python\" ]) Param\u00e8tres avanc\u00e9s generate() model.generate(     input_ids,     max_new_tokens=200,     temperature=0.7,     top_p=0.9,     repetition_penalty=1.1,     do_sample=True ) Gestion GPU / CPU model = AutoModelForCausalLM.from_pretrained(     \"gopu-poss/agentV1\",     device_map=\"cuda\"  # ou \"cpu\" )</p>"},{"location":"architecture/","title":"\ud83d\udcc4 5. <code>docs/architecture.md</code>","text":""},{"location":"architecture/#architecture-de-agentv1","title":"Architecture de agentV1","text":""},{"location":"architecture/#modele-de-base","title":"Mod\u00e8le de base","text":"<ul> <li>Type : Transformer Decoder</li> <li>Param\u00e8tres : ~3.8B</li> <li>Contexte : 4096 tokens</li> <li>Pr\u00e9-entra\u00eenement : corpus multilingue</li> <li>Famille : Microsoft Phi-3 Mini</li> </ul>"},{"location":"architecture/#optimisations-apportees","title":"Optimisations apport\u00e9es","text":"<ul> <li>Quantification FP16</li> <li>Acc\u00e9l\u00e9ration via <code>accelerate</code></li> <li>Memory-efficient attention</li> <li>Tokenizer optimis\u00e9 pour FR/EN</li> <li>Auto device mapping</li> </ul>"},{"location":"architecture/#composants-internes","title":"Composants internes","text":"<ul> <li>Embeddings</li> <li>Multi-head Attention</li> <li>FFN optimis\u00e9</li> <li>Cache KV</li> <li>G\u00e9n\u00e9rateur de tokens</li> </ul>"},{"location":"architecture/#pourquoi-phi-3","title":"Pourquoi Phi-3 ?","text":"<ul> <li>Excellent ratio performance / taille</li> <li>Tr\u00e8s rapide sur GPU modestes</li> <li>Id\u00e9al pour agents, chatbots, assistants \ud83d\udcc4 6. docs/examples.md</li> </ul>"},{"location":"architecture/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"architecture/#resume-de-texte","title":"R\u00e9sum\u00e9 de texte","text":"<p>```python agent.ask(\"R\u00e9sume ce texte : ...\") G\u00e9n\u00e9ration d\u2019histoires agent.ask(     \"\u00c9cris une histoire sur un robot qui apprend l'humour\",     max_tokens=300,     temperature=0.9 ) Aide \u00e0 la programmation agent.ask(\"Montre-moi comment utiliser les dictionnaires en Python.\") Analyse de texte agent.ask(\"Donne-moi 3 points importants du texte suivant : ...\")</p>"},{"location":"deployement/","title":"D\u00e9ploiement","text":""},{"location":"deployement/#gpu-local","title":"GPU local","text":"<pre><code>device_map=\"cuda:0\"\n</code></pre>"},{"location":"deployement/#cpu","title":"CPU","text":"<pre><code>device_map=\"cpu\"\ntorch_dtype=torch.float32\n</code></pre>"},{"location":"deployement/#docker","title":"Docker","text":"<pre><code>FROM python:3.9-slim\nRUN pip install torch transformers accelerate\nCOPY app.py /app/app.py\nCMD [\"python\", \"/app/app.py\"]\n</code></pre>"},{"location":"deployement/#deploiement-sur-serveur","title":"\u2022 D\u00e9ploiement sur serveur","text":""},{"location":"deployement/#uvicorn-fastapi","title":"\u2022 Uvicorn + FastAPI","text":""},{"location":"deployement/#docker-compose","title":"\u2022 Docker compose","text":""},{"location":"deployement/#traefik-nginx","title":"\u2022 Traefik / Nginx","text":""},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequis","title":"Pr\u00e9requis","text":"<ul> <li>Python 3.8+</li> <li>PyTorch 2.0+</li> <li>Transformers 4.25+</li> <li>accelerate (optionnel)</li> </ul>"},{"location":"installation/#installation-des-dependances","title":"Installation des d\u00e9pendances","text":"<pre><code>pip install torch transformers accelerate\n</code></pre>"},{"location":"installation/#telecharger-agentv1","title":"T\u00e9l\u00e9charger agentV1","text":"<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"gopu-poss/agentV1\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"gopu-poss/agentV1\",\n    device_map=\"auto\",\n    torch_dtype=\"auto\"\n)\nV\u00e9rifier l\u2019installation\nprint(model.config)\n</code></pre>"},{"location":"installation/#verification","title":"verification*","text":"<p>```python print(model.config)</p>"},{"location":"models/","title":"\ud83d\udcc4 7. <code>docs/models.md</code>","text":""},{"location":"models/#modeles-disponibles","title":"Mod\u00e8les disponibles","text":""},{"location":"models/#agentv1-stable","title":"agentV1 (stable)","text":"<ul> <li>Base : Phi-3 Mini 4k</li> <li>Multilingue</li> <li>Taille : ~3 Go</li> <li>Optimis\u00e9 FP16</li> </ul>"},{"location":"models/#agentv1-quant-experimental","title":"agentV1-quant (exp\u00e9rimental)","text":"<ul> <li>Quantifi\u00e9 en 4 bits (QLoRA)</li> <li>Ultra l\u00e9ger</li> <li>D\u00e9ployable sur CPU simple</li> </ul>"},{"location":"models/#versions-futures-prevues","title":"Versions futures pr\u00e9vues","text":"<ul> <li>agentV2 (8k tokens)</li> <li>agentV1-vision (multimodal)</li> <li>agent-micro (500M)</li> </ul>"},{"location":"usage/","title":"\ud83d\udcc4 3. <code>docs/usage.md</code>","text":""},{"location":"usage/#utilisation-rapide","title":"Utilisation Rapide","text":""},{"location":"usage/#generation-simple","title":"G\u00e9n\u00e9ration simple","text":"<pre><code>prompt = \"Explique-moi l'IA g\u00e9n\u00e9rative\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=200,\n    temperature=0.7\n)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nMode conversation\nhistory = []\nquestion = \"Bonjour, qui es-tu ?\"\n\nresponse = agent.ask(question)\nParam\u00e8tres importants\nParam\u00e8tre   Description\nmax_new_tokens  Nombre de tokens g\u00e9n\u00e9r\u00e9s\ntemperature Cr\u00e9ativit\u00e9\ntop_p   Filtrage nucleus\ndo_sample   G\u00e9n\u00e9ration al\u00e9atoire\nConseils d\u2019optimisation\nUtiliser torch.float16 sur GPU\nLimiter max_new_tokens sur CPU\nPr\u00e9f\u00e9rer do_sample=True pour la cr\u00e9ativit\u00e9\n</code></pre>"},{"location":"usage/#mode-conversation","title":"Mode conversation","text":"<pre><code>history = []\nquestion = \"Bonjour, qui es-tu ?\"\n\nresponse = agent.ask(question)\nParam\u00e8tres importants\nParam\u00e8tre   Description\nmax_new_tokens  Nombre de tokens g\u00e9n\u00e9r\u00e9s\ntemperature Cr\u00e9ativit\u00e9\ntop_p   Filtrage nucleus\ndo_sample   G\u00e9n\u00e9ration al\u00e9atoire\nConseils d\u2019optimisation\nUtiliser torch.float16 sur GPU\nLimiter max_new_tokens sur CPU\nPr\u00e9f\u00e9rer do_sample=True pour la cr\u00e9ativit\u00e9\n</code></pre>"}]}