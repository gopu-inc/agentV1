{"config":{"lang":["fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Accueil","text":""},{"location":"#agentv1-documentation-officielle","title":"agentV1 \u2014 Documentation Officielle","text":"<p>Bienvenue dans la documentation officielle du mod\u00e8le agentV1, d\u00e9velopp\u00e9 par Mauricio Mangituka pour gopuAI.</p> <p>agentV1 est un mod\u00e8le l\u00e9ger, rapide et optimis\u00e9, reposant sur Microsoft Phi-3-mini-4k-instruct. Il offre des capacit\u00e9s avanc\u00e9es en g\u00e9n\u00e9ration de texte, conversation, analyse et assistance technique.</p>"},{"location":"#fonctionnalites-principales","title":"\ud83d\ude80 Fonctionnalit\u00e9s principales","text":"<ul> <li>Mod\u00e8le l\u00e9ger (~2-3 Go)</li> <li>Multilingue (FR/EN)</li> <li>Raisonnement am\u00e9lior\u00e9</li> <li>Optimisation m\u00e9moire (FP16)</li> <li>Utilisation facile via Transformers</li> <li>API simple d\u2019utilisation</li> <li>D\u00e9ploiement rapide (Docker, Python, CI/CD)</li> </ul>"},{"location":"#contenu-de-la-documentation","title":"\ud83d\udcd8 Contenu de la documentation","text":"<ul> <li>Installation</li> <li>Guide d\u2019utilisation</li> <li>API compl\u00e8te</li> <li>Exemples pratiques</li> <li>Architecture du mod\u00e8le</li> <li>D\u00e9ploiement (CPU, GPU, Docker)</li> <li>FAQ</li> <li>Ressources techniques</li> </ul> <p>D\u00e9velopp\u00e9 avec \u2764\ufe0f par la communaut\u00e9 gopuAI.</p>"},{"location":"about/","title":"\u00c0 propos de gopuAI","text":"<p>gopuAI d\u00e9veloppe des solutions IA performantes, accessibles et enti\u00e8rement ouvertes.</p>"},{"location":"about/#auteur","title":"Auteur","text":"<p>Mauricio Mangituka Cr\u00e9ateur de agentV1 github : @gopu-inc huggingface : gopu-poss mail du admin \ud83d\udcec</p> <p>num du admin\ud83d\udcde</p> <p>hugging agent\ud83e\udd17</p> <p>FAQ\u2049\ufe0f</p>"},{"location":"about/#mission","title":"Mission","text":"<p>Rendre l'intelligence artificielle locale, rapide et accessible \u00e0 tous.</p>"},{"location":"api/","title":"API","text":""},{"location":"api/#apimd","title":"<code>api.md</code>","text":""},{"location":"api/#api-de-agentv1","title":"API de agentV1","text":""},{"location":"api/#classe-principale-agentv1","title":"Classe principale : AgentV1","text":""},{"location":"api/#initialisation","title":"Initialisation","text":"<p>```python</p> <p>agent = AgentV1() M\u00e9thode : ask() Pose une question \u00e0 l\u2019agent. agent.ask(     question=\"Explique-moi Python.\",     max_tokens=200,     temperature=0.7 ) M\u00e9thode : batch_ask() agent.batch_ask([     \"Bonjour\",     \"Explique-moi les listes en Python\" ]) Param\u00e8tres avanc\u00e9s generate() model.generate(     input_ids,     max_new_tokens=200,     temperature=0.7,     top_p=0.9,     repetition_penalty=1.1,     do_sample=True ) Gestion GPU / CPU model = AutoModelForCausalLM.from_pretrained(     \"gopu-poss/agentV1\",     device_map=\"cuda\"  # ou \"cpu\" )</p>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#architecturemd","title":"\ud83d\udcc4 <code>architecture.md</code>","text":""},{"location":"architecture/#architecture-de-agentv1","title":"Architecture de agentV1","text":""},{"location":"architecture/#modele-de-base","title":"Mod\u00e8le de base","text":"<ul> <li>Type : Transformer Decoder</li> <li>Param\u00e8tres : ~3.8B</li> <li>Contexte : 4096 tokens</li> <li>Pr\u00e9-entra\u00eenement : corpus multilingue</li> <li>Famille : Microsoft Phi-3 Mini</li> </ul>"},{"location":"architecture/#optimisations-apportees","title":"Optimisations apport\u00e9es","text":"<ul> <li>Quantification FP16</li> <li>Acc\u00e9l\u00e9ration via <code>accelerate</code></li> <li>Memory-efficient attention</li> <li>Tokenizer optimis\u00e9 pour FR/EN</li> <li>Auto device mapping</li> </ul>"},{"location":"architecture/#composants-internes","title":"Composants internes","text":"<ul> <li>Embeddings</li> <li>Multi-head Attention</li> <li>FFN optimis\u00e9</li> <li>Cache KV</li> <li>G\u00e9n\u00e9rateur de tokens</li> </ul>"},{"location":"architecture/#pourquoi-phi-3","title":"Pourquoi Phi-3 ?","text":"<ul> <li>Excellent ratio performance / taille</li> <li>Tr\u00e8s rapide sur GPU modestes</li> <li>Id\u00e9al pour agents, chatbots, assistants \ud83d\udcc4 6. docs/examples.md</li> </ul>"},{"location":"architecture/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"architecture/#resume-de-texte","title":"R\u00e9sum\u00e9 de texte","text":"<p>```python agent.ask(\"R\u00e9sume ce texte : ...\") G\u00e9n\u00e9ration d\u2019histoires agent.ask(     \"\u00c9cris une histoire sur un robot qui apprend l'humour\",     max_tokens=300,     temperature=0.9 ) Aide \u00e0 la programmation agent.ask(\"Montre-moi comment utiliser les dictionnaires en Python.\") Analyse de texte agent.ask(\"Donne-moi 3 points importants du texte suivant : ...\")</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":""},{"location":"changelog/#v100-premiere-version-stable","title":"v1.0.0 \u2014 Premi\u00e8re version stable","text":"<ul> <li>Publication de agentV1</li> <li>API stable</li> <li>Support FR/EN</li> <li>Optimisation m\u00e9moire</li> <li>Documentation compl\u00e8te cr\u00e9\u00e9e</li> </ul>"},{"location":"changelog/#v090-beta","title":"v0.9.0 \u2014 Beta","text":"<ul> <li>Premi\u00e8re version test</li> <li>Am\u00e9lioration du tokenizer</li> </ul>"},{"location":"deployement/","title":"Deployement","text":""},{"location":"deployement/#deploiement","title":"D\u00e9ploiement","text":""},{"location":"deployement/#gpu-local","title":"GPU local","text":"<pre><code>device_map=\"cuda:0\"\n</code></pre>"},{"location":"deployement/#cpu","title":"CPU","text":"<pre><code>device_map=\"cpu\"\ntorch_dtype=torch.float32\n</code></pre>"},{"location":"deployement/#docker","title":"Docker","text":"<pre><code>FROM python:3.9-slim\nRUN pip install torch transformers accelerate\nCOPY app.py /app/app.py\nCMD [\"python\", \"/app/app.py\"]\n</code></pre>"},{"location":"deployement/#deploiement-sur-serveur","title":"\u2022 D\u00e9ploiement sur serveur","text":""},{"location":"deployement/#uvicorn-fastapi","title":"\u2022 Uvicorn + FastAPI","text":""},{"location":"deployement/#docker-compose","title":"\u2022 Docker compose","text":""},{"location":"deployement/#traefik-nginx","title":"\u2022 Traefik / Nginx","text":""},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#docsfaqmd","title":"\ud83d\udcc4 <code>docs/faq.md</code>","text":""},{"location":"faq/#foire-aux-questions","title":"Foire Aux Questions","text":""},{"location":"faq/#le-modele-est-il-gratuit","title":"Le mod\u00e8le est-il gratuit ?","text":"<p>Oui, licence MIT.</p>"},{"location":"faq/#puis-je-lutiliser-commercialement","title":"Puis-je l\u2019utiliser commercialement ?","text":"<p>Oui.</p>"},{"location":"faq/#le-modele-supporte-quelles-langues","title":"Le mod\u00e8le supporte quelles langues ?","text":"<p>Fran\u00e7ais &amp; Anglais.</p>"},{"location":"faq/#quel-gpu-recommandez-vous","title":"Quel GPU recommandez-vous ?","text":"<ul> <li>RTX 3060+ pour FP16</li> <li>CPU possible mais lent</li> </ul>"},{"location":"faq/#ou-signaler-un-bug","title":"O\u00f9 signaler un bug ?","text":"<p>Sur GitHub Issues.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":""},{"location":"installation/#prerequis","title":"Pr\u00e9requis","text":"<ul> <li>Python 3.8+</li> <li>PyTorch 2.0+</li> <li>Transformers 4.25+</li> <li>accelerate (optionnel)</li> </ul>"},{"location":"installation/#installation-des-dependances","title":"Installation des d\u00e9pendances","text":"<pre><code>pip install torch transformers accelerate\n</code></pre>"},{"location":"installation/#telecharger-agentv1","title":"T\u00e9l\u00e9charger agentV1","text":"<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"gopu-poss/agentV1\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"gopu-poss/agentV1\",\n    device_map=\"auto\",\n    torch_dtype=\"auto\"\n)\nV\u00e9rifier l\u2019installation\nprint(model.config)\n</code></pre>"},{"location":"installation/#verification","title":"verification*","text":"<p>```python print(model.config)</p>"},{"location":"models/","title":"Models","text":""},{"location":"models/#modelsmd","title":"\ud83d\udcc4 <code>models.md</code>","text":""},{"location":"models/#modeles-disponibles","title":"Mod\u00e8les disponibles","text":""},{"location":"models/#agentv1-stable","title":"agentV1 (stable)","text":"<ul> <li>Base : Phi-3 Mini 4k</li> <li>Multilingue</li> <li>Taille : ~3 Go</li> <li>Optimis\u00e9 FP16</li> </ul>"},{"location":"models/#agentv1-quant-experimental","title":"agentV1-quant (exp\u00e9rimental)","text":"<ul> <li>Quantifi\u00e9 en 4 bits (QLoRA)</li> <li>Ultra l\u00e9ger</li> <li>D\u00e9ployable sur CPU simple</li> </ul>"},{"location":"models/#versions-futures-prevues","title":"Versions futures pr\u00e9vues","text":"<ul> <li>agentV2 (8k tokens)</li> <li>agentV1-vision (multimodal)</li> <li>agent-micro (500M)</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#troubleshooting","title":"troubleshooting","text":""},{"location":"troubleshooting/#depannage","title":"D\u00e9pannage","text":""},{"location":"troubleshooting/#probleme-cuda-non-detecte","title":"Probl\u00e8me : CUDA non d\u00e9tect\u00e9","text":"<pre><code>torch.cuda.is_available()\n</code></pre> <p>\u2192 Installer drivers + CUDA Toolkit</p>"},{"location":"troubleshooting/#probleme-out-of-memory","title":"Probl\u00e8me : Out of Memory","text":"<pre><code>Solutions :\n- Baisser max_new_tokens\n- Passer torch_dtype en float16\n- Utiliser CPU offload\n</code></pre>"},{"location":"troubleshooting/#probleme-lenteur-cpu","title":"Probl\u00e8me : Lenteur CPU","text":"<p>Le mod\u00e8le a 3.8B param\u00e8tres \u2192 CPU lent. Utilisez un GPU.</p>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#usagemd","title":"\ud83d\udcc4 <code>usage.md</code>","text":""},{"location":"usage/#utilisation-rapide","title":"Utilisation Rapide","text":""},{"location":"usage/#generation-simple","title":"G\u00e9n\u00e9ration simple","text":"<pre><code>prompt = \"Explique-moi l'IA g\u00e9n\u00e9rative\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=200,\n    temperature=0.7\n)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nMode conversation\nhistory = []\nquestion = \"Bonjour, qui es-tu ?\"\n\nresponse = agent.ask(question)\nParam\u00e8tres importants\nParam\u00e8tre   Description\nmax_new_tokens  Nombre de tokens g\u00e9n\u00e9r\u00e9s\ntemperature Cr\u00e9ativit\u00e9\ntop_p   Filtrage nucleus\ndo_sample   G\u00e9n\u00e9ration al\u00e9atoire\nConseils d\u2019optimisation\nUtiliser torch.float16 sur GPU\nLimiter max_new_tokens sur CPU\nPr\u00e9f\u00e9rer do_sample=True pour la cr\u00e9ativit\u00e9\n</code></pre>"},{"location":"usage/#mode-conversation","title":"Mode conversation","text":"<pre><code>history = []\nquestion = \"Bonjour, qui es-tu ?\"\n\nresponse = agent.ask(question)\nParam\u00e8tres importants\nParam\u00e8tre   Description\nmax_new_tokens  Nombre de tokens g\u00e9n\u00e9r\u00e9s\ntemperature Cr\u00e9ativit\u00e9\ntop_p   Filtrage nucleus\ndo_sample   G\u00e9n\u00e9ration al\u00e9atoire\nConseils d\u2019optimisation\nUtiliser torch.float16 sur GPU\nLimiter max_new_tokens sur CPU\nPr\u00e9f\u00e9rer do_sample=True pour la cr\u00e9ativit\u00e9\n</code></pre>"}]}