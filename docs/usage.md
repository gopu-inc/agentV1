

# üìÑ 3. `docs/usage.md`

# Utilisation Rapide

## G√©n√©ration simple

```python
prompt = "Explique-moi l'IA g√©n√©rative"

inputs = tokenizer(prompt, return_tensors="pt")

outputs = model.generate(
    **inputs,
    max_new_tokens=200,
    temperature=0.7
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
Mode conversation
history = []
question = "Bonjour, qui es-tu ?"

response = agent.ask(question)
Param√®tres importants
Param√®tre	Description
max_new_tokens	Nombre de tokens g√©n√©r√©s
temperature	Cr√©ativit√©
top_p	Filtrage nucleus
do_sample	G√©n√©ration al√©atoire
Conseils d‚Äôoptimisation
Utiliser torch.float16 sur GPU
Limiter max_new_tokens sur CPU
Pr√©f√©rer do_sample=True pour la cr√©ativit√©
```
# *Mode conversation*
```
history = []
question = "Bonjour, qui es-tu ?"

response = agent.ask(question)
Param√®tres importants
Param√®tre	Description
max_new_tokens	Nombre de tokens g√©n√©r√©s
temperature	Cr√©ativit√©
top_p	Filtrage nucleus
do_sample	G√©n√©ration al√©atoire
Conseils d‚Äôoptimisation
Utiliser torch.float16 sur GPU
Limiter max_new_tokens sur CPU
Pr√©f√©rer do_sample=True pour la cr√©ativit√©
```
___
